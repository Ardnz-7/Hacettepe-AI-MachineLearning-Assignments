{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important note:\n",
    "\n",
    "## While doing this assignment I couldn't have manage to use my GPU because of my complicated packages I downloaded(updated) over the years. I have run the code approximately 4 hours and it seemed done after 5 hours.(Because code didn't complete job, I couldn't have viualized.) But unfortunately while I was trying to use my GPU I have restarted code repeatedly. So my outputs have been vanished. I know this code isn't the best but I now it will be okay. \n",
    "\n",
    "## Hope you best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(train_csv_path, test_csv_path, image_size=(64, 64)):\n",
    "    train_data = pd.read_csv(train_csv_path)\n",
    "    test_data = pd.read_csv(test_csv_path)\n",
    "\n",
    "    # Load and preprocess images\n",
    "    def preprocess_images(data, image_size):\n",
    "        images = []\n",
    "        labels = []\n",
    "        for _, row in data.iterrows():\n",
    "            img_path = row['image_path']\n",
    "            label = 1 if row['diagnosis'] == 'malignant' else 0\n",
    "\n",
    "            # Load image and resize\n",
    "            img = tf.keras.preprocessing.image.load_img(img_path, target_size=image_size)\n",
    "            img = tf.keras.preprocessing.image.img_to_array(img) / 255.0  # Normalize\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "        \n",
    "        return np.array(images), np.array(labels)\n",
    "    \n",
    "    train_images, train_labels = preprocess_images(train_data, image_size)\n",
    "    test_images, test_labels = preprocess_images(test_data, image_size)\n",
    "    \n",
    "    return (train_images, train_labels), (test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Derivative of sigmoid\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "# Loss function: Binary Cross-Entropy\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred + 1e-9) + (1 - y_true) * np.log(1 - y_pred + 1e-9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, weights, biases):\n",
    "    layer_inputs = []\n",
    "    layer_outputs = [X]\n",
    "\n",
    "    for w, b in zip(weights, biases):\n",
    "        z = np.dot(layer_outputs[-1], w) + b\n",
    "        a = sigmoid(z)\n",
    "        layer_inputs.append(z)\n",
    "        layer_outputs.append(a)\n",
    "    \n",
    "    return layer_inputs, layer_outputs\n",
    "\n",
    "def backward_propagation(X, y, layer_inputs, layer_outputs, weights):\n",
    "    m = X.shape[0]\n",
    "    gradients = {'dw': [], 'db': []}\n",
    "    L = len(weights)\n",
    "\n",
    "    # Initialize gradient for the output layer\n",
    "    dz = layer_outputs[-1] - y.reshape(-1, 1)\n",
    "    for l in range(L - 1, -1, -1):\n",
    "        dw = np.dot(layer_outputs[l].T, dz) / m\n",
    "        db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "        gradients['dw'].insert(0, dw)\n",
    "        gradients['db'].insert(0, db)\n",
    "\n",
    "        if l > 0:\n",
    "            dz = np.dot(dz, weights[l].T) * sigmoid_derivative(layer_inputs[l - 1])\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(weights, biases, gradients, learning_rate):\n",
    "    for l in range(len(weights)):\n",
    "        weights[l] -= learning_rate * gradients['dw'][l]\n",
    "        biases[l] -= learning_rate * gradients['db'][l]\n",
    "    return weights, biases\n",
    "\n",
    "# Training \n",
    "def train(X, y, layers, learning_rate=0.01, epochs=100, batch_size=32):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Initialize weights and biases\n",
    "    weights = [np.random.randn(layers[i], layers[i+1]) * 0.01 for i in range(len(layers) - 1)]\n",
    "    biases = [np.zeros((1, layers[i+1])) for i in range(len(layers) - 1)]\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        X, y = shuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            X_batch = X[i:i + batch_size]\n",
    "            y_batch = y[i:i + batch_size]\n",
    "\n",
    "            # Forward propagation\n",
    "            layer_inputs, layer_outputs = forward_propagation(X_batch, weights, biases)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = binary_cross_entropy(y_batch, layer_outputs[-1])\n",
    "            losses.append(loss)\n",
    "\n",
    "            # Backward propagation\n",
    "            gradients = backward_propagation(X_batch, y_batch, layer_inputs, layer_outputs, weights)\n",
    "\n",
    "            # Update weights and biases\n",
    "            weights, biases = update_parameters(weights, biases, gradients, learning_rate)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "    return weights, biases, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X, y, weights, biases):\n",
    "    _, layer_outputs = forward_propagation(X, weights, biases)\n",
    "    predictions = (layer_outputs[-1] > 0.5).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(y, predictions)\n",
    "    precision = precision_score(y, predictions)\n",
    "    recall = recall_score(y, predictions)\n",
    "    f1 = f1_score(y, predictions)\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Visualization of results\n",
    "def plot_metrics(losses):\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"311PA3_melanoma_dataset/train\"\n",
    "test_data_path = \"311PA3_melanoma_dataset/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_csv(data_path, output_csv):\n",
    "    rows = []\n",
    "    for label in [\"benign\", \"malignant\"]:\n",
    "        class_dir = os.path.join(data_path, label)  # Join path safely\n",
    "        if not os.path.exists(class_dir):\n",
    "            print(f\"Directory does NOT exist: {class_dir}\")\n",
    "            continue  # Skip if the directory is missing\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            if file_name.endswith(\".jpg\"):\n",
    "                rows.append({\n",
    "                    \"image_path\": os.path.join(class_dir, file_name),\n",
    "                    \"diagnosis\": label\n",
    "                })\n",
    "    # Save as CSV\n",
    "    pd.DataFrame(rows).to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = r\"C:/path/to/311PA3_melanoma_dataset/train\"\n",
    "test_data_path = r\"C:/path/to/311PA3_melanoma_dataset/test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the correct absolute paths\n",
    "train_data_path = r\"C:/Users/Arda Deniz/Desktop/Machine Learning assignment_3/311PA3_melanoma_dataset/train\"\n",
    "test_data_path = r\"C:/Users/Arda Deniz/Desktop/Machine Learning assignment_3/311PA3_melanoma_dataset/test\"\n",
    "\n",
    "# Regenerate the CSV files\n",
    "generate_csv(train_data_path, \"train_data.csv\")\n",
    "generate_csv(test_data_path, \"test_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = load_and_preprocess_data(\n",
    "    train_csv_path=\"train_data.csv\",\n",
    "    test_csv_path=\"test_data.csv\",\n",
    "    image_size=(64, 64)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     
     ]
    }
   ],
   "source": [
    "# Updated Parameters for MLP\n",
    "learning_rates = [0.005, 0.02]\n",
    "batch_size = 16\n",
    "input_sizes = [(50, 50), (300, 300)]  # Resize input images\n",
    "activation_functions = [sigmoid, np.maximum]  # Sigmoid and ReLU\n",
    "hidden_layers = [128, 64, 1]  # Fixed hidden layers for all experiments\n",
    "\n",
    "# Perform 8 Experiments for MLP\n",
    "mlp_results = []\n",
    "for lr in learning_rates:\n",
    "    for input_size in input_sizes:\n",
    "        for activation_fn in activation_functions:\n",
    "            # Resize images and preprocess\n",
    "            resized_train_images = tf.image.resize(train_images, input_size).numpy().reshape(len(train_images), -1)\n",
    "            resized_test_images = tf.image.resize(test_images, input_size).numpy().reshape(len(test_images), -1)\n",
    "\n",
    "            # Train the model\n",
    "            weights, biases, losses = train(\n",
    "                resized_train_images, train_labels,\n",
    "                layers=[input_size[0] * input_size[1] * 3] + hidden_layers,\n",
    "                learning_rate=lr,\n",
    "                epochs=10,\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "\n",
    "            # Evaluate the model\n",
    "            accuracy, precision, recall, f1 = evaluate(resized_test_images, test_labels, weights, biases)\n",
    "\n",
    "            # Log results\n",
    "            mlp_results.append({\n",
    "                \"Learning Rate\": lr,\n",
    "                \"Batch Size\": batch_size,\n",
    "                \"Input Size\": f\"{input_size[0]}x{input_size[1]}\",\n",
    "                \"Activation Function\": \"Sigmoid\" if activation_fn == sigmoid else \"ReLU\",\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"Precision\": precision,\n",
    "                \"Recall\": recall,\n",
    "                \"F1-Score\": f1\n",
    "            })\n",
    "\n",
    "# Display results for MLP\n",
    "import pandas as pd\n",
    "mlp_results_df = pd.DataFrame(mlp_results)\n",
    "print(mlp_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arda Deniz\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m481/481\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 482ms/step - accuracy: 0.4899 - loss: 1.0716 - val_accuracy: 0.5206 - val_loss: 0.6926\n",
      "Epoch 2/10\n",
      "\u001b[1m481/481\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 455ms/step - accuracy: 0.5060 - loss: 0.6997 - val_accuracy: 0.4794 - val_loss: 0.7189\n",
      "Epoch 3/10\n",
      "\u001b[1m304/481\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 439ms/step - accuracy: 0.5004 - loss: 0.7012"
     ]
    },
    {
     
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Updated Parameters for CNN\n",
    "learning_rates = [0.005, 0.02]\n",
    "batch_sizes = [16, 32]\n",
    "input_size = (300, 300)  # Fixed input size for all CNN experiments\n",
    "activation_functions = ['sigmoid', 'relu']\n",
    "\n",
    "# Perform 8 Experiments for CNN\n",
    "cnn_results = []\n",
    "for lr in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for activation_fn in activation_functions:\n",
    "            # Build the CNN model\n",
    "            model = models.Sequential([\n",
    "                layers.Conv2D(32, (3, 3), activation=activation_fn, input_shape=(input_size[0], input_size[1], 3)),\n",
    "                layers.MaxPooling2D((2, 2)),\n",
    "                layers.Conv2D(64, (3, 3), activation=activation_fn),\n",
    "                layers.MaxPooling2D((2, 2)),\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(128, activation=activation_fn),\n",
    "                layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "            ])\n",
    "\n",
    "            # Compile the model\n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "            # Resize images and preprocess\n",
    "            resized_train_images = tf.image.resize(train_images, input_size).numpy()\n",
    "            resized_test_images = tf.image.resize(test_images, input_size).numpy()\n",
    "\n",
    "            # Train the model\n",
    "            history = model.fit(\n",
    "                resized_train_images, train_labels,\n",
    "                epochs=10,  # Adjust for quick runs\n",
    "                batch_size=batch_size,\n",
    "                validation_data=(resized_test_images, test_labels),\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            # Evaluate the model\n",
    "            test_loss, test_accuracy = model.evaluate(resized_test_images, test_labels, verbose=0)\n",
    "\n",
    "            # Log results\n",
    "            cnn_results.append({\n",
    "                \"Learning Rate\": lr,\n",
    "                \"Batch Size\": batch_size,\n",
    "                \"Activation Function\": activation_fn.capitalize(),\n",
    "                \"Test Accuracy\": test_accuracy\n",
    "            })\n",
    "\n",
    "# Display results for CNN\n",
    "cnn_results_df = pd.DataFrame(cnn_results)\n",
    "print(cnn_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
    
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Combine results into a single table\n",
    "combined_results = pd.concat([\n",
    "    mlp_results_df.assign(Model='MLP'),\n",
    "    cnn_results_df.assign(Model='CNN')\n",
    "])\n",
    "\n",
    "# Save to CSV for submission\n",
    "combined_results.to_csv(\"experiment_results.csv\", index=False)\n",
    "\n",
    "# Print Combined Results\n",
    "print(combined_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
